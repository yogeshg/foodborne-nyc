* Finalize for yelp
  - each experiment, settings, results, justification, conclusion
  - 
  - 
* Story
  - Baseline : Logistic regression
  - baseline : Wordvecs : sum up 
  - freeze word vecs
  - convolutions (in passing mention: regularize with dropouts, L2)
  - higher n-grams and number of filters

* interpretability, introspection
  - looking inside the model
  - fix that sigma thing

* our datasets are built from biased data
  - does work on the 
  - retrofit for original yelp feed
  - by: augment the data 1000 examples from yelp feed chosen at random
    + 

* do this all for twitter




data1 : 13k
data2 : 1k / 5
data3 : sample new 1k (call them -ve)


regression-baseline
 -- trained on data1 (90%)
 -- evaluate on data2 (~82%)
 -- trained on data1+data2 == data3
 -- evaluate on the 