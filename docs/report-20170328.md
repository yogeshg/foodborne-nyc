
<!--  * does dropout help?
 * making all embeddings unit length seem to help
	![](trigrams10.png)
	![](trigrams10_unit.png)
	![](trigrams25.png)
	![](trigrams25_unit.png)
 * 

# current_20170327*
  - dropout -->

# [old results](file:///Users/yogi/data/fbnyc/archive_trial2_1/view.html)
  - 301 - 4521 params: accuracies between 0.70-0.75
  - accuracies increasing for bigger models
  - 90,301 params : 0.81


# current_20170328_003506
  - studying learning rate
    + [0.0001](file:///Users/yogi/data/fbnyc/archive/current_20170328_003506/f_0.0001/view.html)
    + [0.001](file:///Users/yogi/data/fbnyc/archive/current_20170328_003506/f_0.001/view.html)
    + [1e-05](file:///Users/yogi/data/fbnyc/archive/current_20170328_003506/f_1e-05/view.html)
  - other parameters:
    + dropout
    + unitizing the word embeddings
  - has different learning rates
    + 0.001 seems good enough
    + 0.0001 and 1e-5 are too less, will need to increase patience
    + smaller lr are better for more parameters!
  - dropout(0.5) seems to get much better results need to compare with others

# [current_20170329_020625](file:///Users/yogi/data/fbnyc/archive/current_20170329_020625/view.html)
  - compares embeddings
  - unitizing seems to hurt for 5,10 filters

  | trainable   |   none  |  l2(0.001) |
  |-------------|---------|------------|
  | 9031        |   0.80  |    0.79    |
  | 18061       |   0.82  |    0.80    |
  | 45151       |    ??   |    0.79    |
  | 90301       |    ??   |    0.80    |

# [current_20170329_224341 current_20170330_012413](file:///Users/yogi/data/fbnyc/archive/current_20170330_012413/view.html)
  - ignore the first two
    + l2 regularization gives way off results with logsumexp
    + it was probably too big regularizer 0.01 or logsumexp
  - changed to 0.001 and max
  - 9031.0 and max : 0.81 -> 0.80
  - 18061.0 and max : 0.79 -> 0.81

# [current_20170330_014227](file:///Users/yogi/data/fbnyc/archive/current_20170330_014227/view.html)
  - testing pooling layers
  - with and without l2 regularisation with 0.001

  | function    |   none  |  l2(0.001) |
  |-------------|---------|------------|
  | sum         |   0.81  |    0.81    |
  | avg         |   0.76  |    0.77    |
  | logsumexp   |   0.82  |     ??     |

  - l2(0.01) with logsumexp and unitized vectors had
    validation accuracy all over the place

# [current_20170329_183152 current_20170330_022115](file:///Users/yogi/data/fbnyc/archive/current_20170330_022115/view.html)
  - logsumexp changes the learning pattern
  - max before 100, big dip that seems to increase, after 500
  - max pooling seems better than average
  - waiting for results of logsumexp


<!-- dropout - more?
normalization of embeddings

logsumexp
l2 regularization
past 3 grams
 -->
